{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder(P2P) --> Diffusion(DDPM) --> Decoder(P2P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anshul/anaconda3/envs/bhavik_mini/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torchvision.transforms as transforms\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "from diffusers import DDPMScheduler\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torchinfo import summary\n",
    "import torch.cuda.amp as amp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Created directory: ./outputs/pipeline_samples\n",
      "üìÅ Created directory: ./outputs/pipeline_checkpoints\n",
      "üìÅ Created directory: ./outputs/pipeline_plots\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    \"project_name\": \"Pix2Pix_Diffusion_Pipeline\",\n",
    "    \"device\": \"cuda:1\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \"batch_size\": 512,\n",
    "    \"timesteps\": 1000,\n",
    "    \"embedding_dim\": 512,\n",
    "    \"time_emb_dim\": 256,\n",
    "    \"learning_rate\": 2e-4,\n",
    "    \"num_epochs\": 50,\n",
    "    \"save_checkpoint_interval\": 5,\n",
    "    \"diffusion_loss_weight\": 0.65,\n",
    "    \"latent_loss_weight\": 0.1,\n",
    "    \"sample_interval\": 10,\n",
    "    \"val_interval\": 10,\n",
    "    \"use_mixed_precision\": True,\n",
    "    \"logging\": {\n",
    "        \"use_wandb\": True,\n",
    "        \"sample_dir\": \"./outputs/pipeline_samples\",\n",
    "        \"checkpoint_dir\": \"./outputs/pipeline_checkpoints\",\n",
    "        \"plot_dir\": \"./outputs/pipeline_plots\"\n",
    "    }\n",
    "}\n",
    "\n",
    "for dir_path in [config[\"logging\"][\"sample_dir\"], config[\"logging\"][\"checkpoint_dir\"], config[\"logging\"][\"plot_dir\"]]:\n",
    "    try:\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "        print(f\"üìÅ Created directory: {dir_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Failed to create directory {dir_path}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DDPMScheduler\n",
    "import math\n",
    "\n",
    "class DiffusionAutoencoder(nn.Module):\n",
    "    \"\"\"Diffusion Autoencoder combining an encoder, U-Net, and decoder for image reconstruction.\"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.device = torch.device(config[\"device\"])\n",
    "        self.embedding_dim = config[\"embedding_dim\"]\n",
    "        self.time_emb_dim = config[\"time_emb_dim\"]\n",
    "        self.timesteps = config[\"timesteps\"]\n",
    "        \n",
    "        self.encoder = self.Encoder(in_channels=3, features=64, embedding_dim=self.embedding_dim)\n",
    "        self.unet = self.ImprovedMLP_UNet(embedding_dim=self.embedding_dim, time_dim=self.time_emb_dim)\n",
    "        self.decoder = self.Decoder(out_channels=3, features=64, embedding_dim=self.embedding_dim)\n",
    "        self.scheduler = self.DiffusionScheduler(timesteps=self.timesteps)\n",
    "        \n",
    "        self.to(self.device)\n",
    "\n",
    "    class SinusoidalPositionEmbeddings(nn.Module):\n",
    "        \"\"\"Generates sinusoidal position embeddings for timesteps.\"\"\"\n",
    "        def __init__(self, dim):\n",
    "            super().__init__()\n",
    "            self.dim = dim\n",
    "\n",
    "        def forward(self, time):\n",
    "            device = time.device\n",
    "            half_dim = self.dim // 2\n",
    "            embeddings = math.log(10000) / (half_dim - 1)\n",
    "            embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
    "            embeddings = time[:, None] * embeddings[None, :]\n",
    "            embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
    "            if self.dim % 2 == 1:\n",
    "                embeddings = F.pad(embeddings, (0, 1, 0, 0))\n",
    "            return embeddings\n",
    "\n",
    "    class Encoder(nn.Module):\n",
    "        \"\"\"Encodes input images into a latent representation.\"\"\"\n",
    "        def __init__(self, in_channels, features, embedding_dim):\n",
    "            super().__init__()\n",
    "            self.initial = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, features, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "                nn.LeakyReLU(0.2, inplace=True)\n",
    "            )\n",
    "            self.down1 = self._block(features, features * 2)\n",
    "            self.down2 = self._block(features * 2, features * 4)\n",
    "            self.down3 = self._block(features * 4, features * 8)\n",
    "            self.down4 = self._block(features * 8, features * 8)\n",
    "            self.final = nn.Sequential(\n",
    "                nn.AdaptiveAvgPool2d(1),\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(features * 8, embedding_dim)\n",
    "            )\n",
    "\n",
    "        def _block(self, in_channels, out_channels):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.LeakyReLU(0.2, inplace=True)\n",
    "            )\n",
    "\n",
    "        def forward(self, x):\n",
    "            d1 = self.initial(x)\n",
    "            d2 = self.down1(d1)\n",
    "            d3 = self.down2(d2)\n",
    "            d4 = self.down3(d3)\n",
    "            d5 = self.down4(d4)\n",
    "            embedding = self.final(d5)\n",
    "            return embedding, [d1, d2, d3, d4, d5]\n",
    "\n",
    "    class ResidualBlock(nn.Module):\n",
    "        \"\"\"Residual block for U-Net with timestep embeddings.\"\"\"\n",
    "        def __init__(self, in_channels, out_channels, time_emb_dim, dropout=0.1):\n",
    "            super().__init__()\n",
    "            self.time_mlp = nn.Sequential(\n",
    "                nn.Linear(time_emb_dim, out_channels),\n",
    "                nn.GELU()\n",
    "            )\n",
    "            self.block1 = nn.Sequential(\n",
    "                nn.Linear(in_channels, out_channels),\n",
    "                nn.GELU(),\n",
    "                nn.Dropout(dropout)\n",
    "            )\n",
    "            self.block2 = nn.Sequential(\n",
    "                nn.Linear(out_channels, out_channels),\n",
    "                nn.GELU(),\n",
    "                nn.Dropout(dropout)\n",
    "            )\n",
    "            self.residual_conv = nn.Linear(in_channels, out_channels) if in_channels != out_channels else nn.Identity()\n",
    "            self.layer_norm = nn.LayerNorm(out_channels)\n",
    "\n",
    "        def forward(self, x, t):\n",
    "            h = self.block1(x)\n",
    "            time_emb = self.time_mlp(t)\n",
    "            h = h + time_emb\n",
    "            h = self.block2(h)\n",
    "            return self.layer_norm(h + self.residual_conv(x))\n",
    "\n",
    "    class ImprovedMLP_UNet(nn.Module):\n",
    "        \"\"\"U-Net model for denoising latent representations with timestep conditioning.\"\"\"\n",
    "        def __init__(self, embedding_dim, hidden_dim=1024, time_dim=256, dropout=0.1):\n",
    "            super().__init__()\n",
    "            self.time_mlp = nn.Sequential(\n",
    "                DiffusionAutoencoder.SinusoidalPositionEmbeddings(time_dim),\n",
    "                nn.Linear(time_dim, time_dim * 2),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(time_dim * 2, time_dim),\n",
    "            )\n",
    "            self.down1 = DiffusionAutoencoder.ResidualBlock(embedding_dim, hidden_dim, time_dim, dropout)\n",
    "            self.down2 = DiffusionAutoencoder.ResidualBlock(hidden_dim, hidden_dim, time_dim, dropout)\n",
    "            self.mid = DiffusionAutoencoder.ResidualBlock(hidden_dim, hidden_dim, time_dim, dropout)\n",
    "            self.up1 = DiffusionAutoencoder.ResidualBlock(hidden_dim * 2, hidden_dim, time_dim, dropout)\n",
    "            self.up2 = DiffusionAutoencoder.ResidualBlock(hidden_dim * 2, embedding_dim, time_dim, dropout)\n",
    "            self.final = nn.Sequential(\n",
    "                nn.Linear(embedding_dim, embedding_dim),\n",
    "                nn.Tanh()\n",
    "            )\n",
    "\n",
    "        def forward(self, x, t):\n",
    "            time_emb = self.time_mlp(t)\n",
    "            down1 = self.down1(x, time_emb)\n",
    "            down2 = self.down2(down1, time_emb)\n",
    "            mid = self.mid(down2, time_emb)\n",
    "            up1 = self.up1(torch.cat([mid, down2], dim=1), time_emb)\n",
    "            up2 = self.up2(torch.cat([up1, down1], dim=1), time_emb)\n",
    "            return self.final(up2)\n",
    "\n",
    "    class Decoder(nn.Module):\n",
    "        \"\"\"Decodes latent representations back to images.\"\"\"\n",
    "        def __init__(self, out_channels, features, embedding_dim):\n",
    "            super().__init__()\n",
    "            self.project = nn.Sequential(\n",
    "                nn.Linear(embedding_dim, 512 * 2 * 2),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "            self.up1 = self._block(512, 512)\n",
    "            self.up2 = self._block(512 + 512, 256)\n",
    "            self.up3 = self._block(256 + 256, 128)\n",
    "            self.up4 = self._block(128 + 128, 64)\n",
    "            self.final = nn.Sequential(\n",
    "                nn.ConvTranspose2d(64 + 64, out_channels, kernel_size=4, stride=2, padding=1),\n",
    "                nn.Tanh()\n",
    "            )\n",
    "\n",
    "        def _block(self, in_channels, out_channels):\n",
    "            return nn.Sequential(\n",
    "                nn.ConvTranspose2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "\n",
    "        def forward(self, x, encoder_features=None):\n",
    "            x = self.project(x)\n",
    "            x = x.view(-1, 512, 2, 2)\n",
    "            x1 = self.up1(x)\n",
    "            if encoder_features:\n",
    "                x2 = self.up2(torch.cat([x1, encoder_features[3]], dim=1))\n",
    "                x3 = self.up3(torch.cat([x2, encoder_features[2]], dim=1))\n",
    "                x4 = self.up4(torch.cat([x3, encoder_features[1]], dim=1))\n",
    "                output = self.final(torch.cat([x4, encoder_features[0]], dim=1))\n",
    "            else:\n",
    "                output = self.final(x1)\n",
    "            return output\n",
    "\n",
    "    class DiffusionScheduler:\n",
    "        \"\"\"Manages noise scheduling for diffusion process using DDPMScheduler.\"\"\"\n",
    "        def __init__(self, timesteps):\n",
    "            self.scheduler = DDPMScheduler(num_train_timesteps=timesteps)\n",
    "\n",
    "        def add_noise(self, x, t):\n",
    "            noise = torch.randn_like(x)\n",
    "            noisy_x = self.scheduler.add_noise(x, noise, t)\n",
    "            return noisy_x, noise\n",
    "\n",
    "    def forward(self, img, t):\n",
    "        \"\"\"Forward pass through encoder, U-Net, and decoder.\"\"\"\n",
    "        latent, encoder_features = self.encoder(img)\n",
    "        noisy_latent, noise = self.scheduler.add_noise(latent, t)\n",
    "        denoised_latent = self.unet(noisy_latent, t)\n",
    "        reconstructed_img = self.decoder(denoised_latent, encoder_features)\n",
    "        return reconstructed_img, noise, latent, denoised_latent\n",
    "\n",
    "\n",
    "\n",
    "# purifier = DiffusionAutoencoder(config).to(config['device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sample_images(model, loader, epoch, config, prefix=\"train\", num_samples=4):\n",
    "    \"\"\"Generate and save sample images, logging to WandB.\"\"\"\n",
    "    model.eval()\n",
    "    sample_dir = config[\"logging\"][\"sample_dir\"]\n",
    "    wandb_images = []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            images = batch['image'][:num_samples].to(model.device)\n",
    "            t = torch.randint(0, model.timesteps, (num_samples,), device=model.device).long()\n",
    "            output, _, _, _ = model(images, t)\n",
    "            output = output.cpu().numpy().transpose(0, 2, 3, 1)\n",
    "            images = images.cpu().numpy().transpose(0, 2, 3, 1)\n",
    "\n",
    "            output = (output * 0.5 + 0.5).clip(0, 1)\n",
    "            images = (images * 0.5 + 0.5).clip(0, 1)\n",
    "\n",
    "            for i in range(num_samples):\n",
    "                fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(6, 3))\n",
    "                ax1.imshow(images[i])\n",
    "                ax1.set_title(\"Input\")\n",
    "                ax1.axis(\"off\")\n",
    "                ax2.imshow(output[i])\n",
    "                ax2.set_title(\"Reconstructed\")\n",
    "                ax2.axis(\"off\")\n",
    "\n",
    "                sample_path = os.path.join(sample_dir, f\"{prefix}_epoch_{epoch}_sample_{i}.png\")\n",
    "                plt.savefig(sample_path, bbox_inches=\"tight\")\n",
    "                plt.close(fig)\n",
    "                print(f\"üì∏ Saved {prefix} sample: {sample_path}\")\n",
    "\n",
    "                # if config[\"logging\"][\"use_wandb\"]:\n",
    "                #     wandb_images.append(wandb.Image(sample_path, caption=f\"{prefix.capitalize()} Epoch {epoch} Sample {i}\"))\n",
    "\n",
    "            # if config[\"logging\"][\"use_wandb\"] and wandb_images:\n",
    "            #     wandb.log({f\"{prefix}_samples\": wandb_images}, commit=False)\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def validate_model(model, val_loader, config, epoch):\n",
    "    \"\"\"Evaluate model on validation set and return loss metrics.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_recon_loss = 0\n",
    "    total_diffusion_loss = 0\n",
    "    total_latent_loss = 0\n",
    "    num_batches = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            images = batch['image'].to(model.device)\n",
    "            t = torch.randint(0, model.timesteps, (images.size(0),), device=model.device).long()\n",
    "            output, noise, latent, denoised_latent = model(images, t)\n",
    "\n",
    "            recon_loss = F.mse_loss(output, batch['target'].to(model.device))\n",
    "            diffusion_loss = F.mse_loss(denoised_latent, latent)\n",
    "            latent_loss = F.mse_loss(denoised_latent, latent.detach())\n",
    "            loss = recon_loss + config[\"diffusion_loss_weight\"] * diffusion_loss + config[\"latent_loss_weight\"] * latent_loss\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_recon_loss += recon_loss.item()\n",
    "            total_diffusion_loss += diffusion_loss.item()\n",
    "            total_latent_loss += latent_loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "    avg_loss = total_loss / num_batches\n",
    "    avg_recon_loss = total_recon_loss / num_batches\n",
    "    avg_diffusion_loss = total_diffusion_loss / num_batches\n",
    "    avg_latent_loss = total_latent_loss / num_batches\n",
    "\n",
    "    metrics = {\n",
    "        \"val_loss\": avg_loss,\n",
    "        \"val_recon_loss\": avg_recon_loss,\n",
    "        \"val_diffusion_loss\": avg_diffusion_loss,\n",
    "        \"val_latent_loss\": avg_latent_loss\n",
    "    }\n",
    "    return metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, config, optimizer=None):\n",
    "    \"\"\"Train the diffusion autoencoder with mixed precision and validation.\"\"\"\n",
    "    if optimizer is None:\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=config[\"learning_rate\"])\n",
    "\n",
    "    scaler = amp.GradScaler(enabled=config[\"use_mixed_precision\"])\n",
    "    \n",
    "    # if config[\"logging\"][\"use_wandb\"]:\n",
    "        # try:\n",
    "        #     wandb.init(project=config[\"project_name\"], config=config)\n",
    "        #     print(\"‚úÖ WandB initialized\")\n",
    "        # except Exception as e:\n",
    "        #     print(f\"‚ö†Ô∏è WandB initialization failed: {e}\")\n",
    "        #     config[\"logging\"][\"use_wandb\"] = False\n",
    "\n",
    "    for epoch in tqdm(range(config[\"num_epochs\"]), desc=\"Epochs\", colour=\"green\"):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        total_recon_loss = 0\n",
    "        total_diffusion_loss = 0\n",
    "        total_latent_loss = 0\n",
    "        num_batches = 0\n",
    "\n",
    "        for batch in tqdm(train_loader, desc=\"Batches\", leave=False):\n",
    "            images = batch['image'].to(model.device)\n",
    "            t = torch.randint(0, model.timesteps, (images.size(0),), device=model.device).long()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            with amp.autocast(enabled=config[\"use_mixed_precision\"]):\n",
    "                output, noise, latent, denoised_latent = model(images, t)\n",
    "                recon_loss = F.mse_loss(output, batch['target'].to(model.device))\n",
    "                diffusion_loss = F.mse_loss(denoised_latent, latent)\n",
    "                latent_loss = F.mse_loss(denoised_latent, latent.detach())\n",
    "                loss = recon_loss + config[\"diffusion_loss_weight\"] * diffusion_loss + config[\"latent_loss_weight\"] * latent_loss\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_recon_loss += recon_loss.item()\n",
    "            total_diffusion_loss += diffusion_loss.item()\n",
    "            total_latent_loss += latent_loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "            # if config[\"logging\"][\"use_wandb\"]:\n",
    "            #     wandb.log({\n",
    "            #         \"train_loss\": loss.item(),\n",
    "            #         \"train_recon_loss\": recon_loss.item(),\n",
    "            #         \"train_diffusion_loss\": diffusion_loss.item(),\n",
    "            #         \"train_latent_loss\": latent_loss.item()\n",
    "            #     }, commit=False)\n",
    "\n",
    "        avg_loss = total_loss / num_batches\n",
    "        print(f\"Epoch {epoch + 1}/{config['num_epochs']}, \"\n",
    "              f\"Train Loss: {avg_loss:.4f}, \"\n",
    "              f\"Recon: {total_recon_loss / num_batches:.4f}, \"\n",
    "              f\"Diffusion: {total_diffusion_loss / num_batches:.4f}, \"\n",
    "              f\"Latent: {total_latent_loss / num_batches:.4f}\")\n",
    "\n",
    "        if (epoch + 1) % config[\"val_interval\"] == 0:\n",
    "            val_metrics = validate_model(model, val_loader, config, epoch + 1)\n",
    "            print(f\"Validation - Loss: {val_metrics['val_loss']:.4f}, \"\n",
    "                  f\"Recon: {val_metrics['val_recon_loss']:.4f}, \"\n",
    "                  f\"Diffusion: {val_metrics['val_diffusion_loss']:.4f}, \"\n",
    "                  f\"Latent: {val_metrics['val_latent_loss']:.4f}\")\n",
    "            # if config[\"logging\"][\"use_wandb\"]:\n",
    "            #     wandb.log(val_metrics, commit=False)\n",
    "\n",
    "        if (epoch + 1) % config[\"sample_interval\"] == 0:\n",
    "            sample_images(model, train_loader, epoch + 1, config, prefix=\"train\")\n",
    "            sample_images(model, val_loader, epoch + 1, config, prefix=\"val\")\n",
    "\n",
    "        if (epoch + 1) % config[\"save_checkpoint_interval\"] == 0:\n",
    "            checkpoint_path = os.path.join(config[\"logging\"][\"checkpoint_dir\"], f\"model_epoch_{epoch + 1}.pth\")\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "            print(f\"üíæ Saved checkpoint: {checkpoint_path}\")\n",
    "\n",
    "        # if config[\"logging\"][\"use_wandb\"]:\n",
    "        #     wandb.log({\"epoch\": epoch + 1, \"avg_train_loss\": avg_loss}, commit=True)\n",
    "\n",
    "    final_checkpoint_path = os.path.join(config[\"logging\"][\"checkpoint_dir\"], \"model_final.pth\")\n",
    "    torch.save(model.state_dict(), final_checkpoint_path)\n",
    "    print(f\"üíæ Saved final checkpoint: {final_checkpoint_path}\")\n",
    "    print(\"‚úÖ Training completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImagenetDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, transform=None):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        image = item['image']\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        noised_image = image + torch.randn_like(image) * torch.rand(1).item()\n",
    "        return {'image': noised_image, 'target': image}\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Lambda(lambda img: img.convert(\"RGB\")),\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4802, 0.4481, 0.3975), (0.2302, 0.2265, 0.2262)),\n",
    "])\n",
    "\n",
    "dataset = load_dataset(\"zh-plus/tiny-imagenet\", split=\"train\")\n",
    "dataset = ImagenetDataset(dataset, transform=transform)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=config[\"batch_size\"], shuffle=False, num_workers=4)\n",
    "# from torch.utils.data import Dataset\n",
    "# class ImageNetDataset(Dataset):\n",
    "#     def __init__(self, root_dir, transform=None):\n",
    "#         self.root_dir = root_dir\n",
    "#         self.transform = transform\n",
    "#         self.image_paths = [os.path.join(root, file) for root, _, files in os.walk(root_dir) for file in files if file.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.image_paths)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         img_path = self.image_paths[idx]\n",
    "#         image = Image.open(img_path).convert(\"RGB\")\n",
    "#         if self.transform:\n",
    "#             image = self.transform(image)\n",
    "\n",
    "#         noised_image = image+torch.randn_like(image)\n",
    "#         return {'image':noised_image, 'target':image}\n",
    "\n",
    "\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize((224, 224)),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize((0.4802, 0.4481, 0.3975), (0.2302, 0.2265, 0.2262)),\n",
    "# ])\n",
    "\n",
    "# imagenet_dir = \"./val\"\n",
    "# dataset = ImageNetDataset(root_dir=imagenet_dir, transform=transform)\n",
    "# train_size = int(0.8 * len(dataset))\n",
    "# val_size = len(dataset) - train_size\n",
    "# train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True, num_workers=4)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=config[\"batch_size\"], shuffle=False, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DiffusionAutoencoder(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1340133/2310151332.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = amp.GradScaler(enabled=config[\"use_mixed_precision\"])\n",
      "Epochs:   0%|\u001b[32m          \u001b[0m| 0/50 [00:00<?, ?it/s]/tmp/ipykernel_1340133/2310151332.py:29: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(enabled=config[\"use_mixed_precision\"]):\n",
      "Epochs:   2%|\u001b[32m‚ñè         \u001b[0m| 1/50 [00:09<07:59,  9.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Train Loss: 0.4991, Recon: 0.4912, Diffusion: 0.0104, Latent: 0.0104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   4%|\u001b[32m‚ñç         \u001b[0m| 2/50 [00:18<07:07,  8.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50, Train Loss: 0.3342, Recon: 0.3339, Diffusion: 0.0004, Latent: 0.0004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   6%|\u001b[32m‚ñå         \u001b[0m| 3/50 [00:26<06:40,  8.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/50, Train Loss: 0.3180, Recon: 0.3178, Diffusion: 0.0002, Latent: 0.0002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   8%|\u001b[32m‚ñä         \u001b[0m| 4/50 [00:34<06:26,  8.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/50, Train Loss: 0.3098, Recon: 0.3096, Diffusion: 0.0002, Latent: 0.0002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/50, Train Loss: 0.3050, Recon: 0.3049, Diffusion: 0.0001, Latent: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  10%|\u001b[32m‚ñà         \u001b[0m| 5/50 [00:44<06:39,  8.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saved checkpoint: ./outputs/pipeline_checkpoints/model_epoch_5.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  12%|\u001b[32m‚ñà‚ñè        \u001b[0m| 6/50 [00:52<06:19,  8.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/50, Train Loss: 0.3021, Recon: 0.3021, Diffusion: 0.0001, Latent: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  14%|\u001b[32m‚ñà‚ñç        \u001b[0m| 7/50 [01:00<06:00,  8.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/50, Train Loss: 0.2995, Recon: 0.2995, Diffusion: 0.0001, Latent: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  16%|\u001b[32m‚ñà‚ñå        \u001b[0m| 8/50 [01:08<05:47,  8.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/50, Train Loss: 0.2974, Recon: 0.2974, Diffusion: 0.0001, Latent: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  18%|\u001b[32m‚ñà‚ñä        \u001b[0m| 9/50 [01:17<05:54,  8.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/50, Train Loss: 0.2960, Recon: 0.2960, Diffusion: 0.0001, Latent: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50, Train Loss: 0.2946, Recon: 0.2945, Diffusion: 0.0001, Latent: 0.0001\n",
      "Validation - Loss: 0.2946, Recon: 0.2946, Diffusion: 0.0000, Latent: 0.0000\n",
      "üì∏ Saved train sample: ./outputs/pipeline_samples/train_epoch_10_sample_0.png\n",
      "üì∏ Saved train sample: ./outputs/pipeline_samples/train_epoch_10_sample_1.png\n",
      "üì∏ Saved train sample: ./outputs/pipeline_samples/train_epoch_10_sample_2.png\n",
      "üì∏ Saved train sample: ./outputs/pipeline_samples/train_epoch_10_sample_3.png\n",
      "üì∏ Saved val sample: ./outputs/pipeline_samples/val_epoch_10_sample_0.png\n",
      "üì∏ Saved val sample: ./outputs/pipeline_samples/val_epoch_10_sample_1.png\n",
      "üì∏ Saved val sample: ./outputs/pipeline_samples/val_epoch_10_sample_2.png\n",
      "üì∏ Saved val sample: ./outputs/pipeline_samples/val_epoch_10_sample_3.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  20%|\u001b[32m‚ñà‚ñà        \u001b[0m| 10/50 [01:29<06:31,  9.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saved checkpoint: ./outputs/pipeline_checkpoints/model_epoch_10.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  22%|\u001b[32m‚ñà‚ñà‚ñè       \u001b[0m| 11/50 [01:37<05:59,  9.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/50, Train Loss: 0.2934, Recon: 0.2933, Diffusion: 0.0000, Latent: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  24%|\u001b[32m‚ñà‚ñà‚ñç       \u001b[0m| 12/50 [01:45<05:37,  8.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/50, Train Loss: 0.2922, Recon: 0.2921, Diffusion: 0.0000, Latent: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  26%|\u001b[32m‚ñà‚ñà‚ñå       \u001b[0m| 13/50 [01:55<05:34,  9.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/50, Train Loss: 0.2910, Recon: 0.2909, Diffusion: 0.0000, Latent: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  28%|\u001b[32m‚ñà‚ñà‚ñä       \u001b[0m| 14/50 [02:03<05:14,  8.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/50, Train Loss: 0.2906, Recon: 0.2906, Diffusion: 0.0000, Latent: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/50, Train Loss: 0.2896, Recon: 0.2896, Diffusion: 0.0000, Latent: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  30%|\u001b[32m‚ñà‚ñà‚ñà       \u001b[0m| 15/50 [02:11<05:02,  8.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saved checkpoint: ./outputs/pipeline_checkpoints/model_epoch_15.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  32%|\u001b[32m‚ñà‚ñà‚ñà‚ñè      \u001b[0m| 16/50 [02:20<04:49,  8.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/50, Train Loss: 0.2888, Recon: 0.2887, Diffusion: 0.0000, Latent: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  34%|\u001b[32m‚ñà‚ñà‚ñà‚ñç      \u001b[0m| 17/50 [02:29<04:51,  8.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/50, Train Loss: 0.2881, Recon: 0.2881, Diffusion: 0.0000, Latent: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  36%|\u001b[32m‚ñà‚ñà‚ñà‚ñå      \u001b[0m| 18/50 [02:37<04:35,  8.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/50, Train Loss: 0.2875, Recon: 0.2875, Diffusion: 0.0000, Latent: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  38%|\u001b[32m‚ñà‚ñà‚ñà‚ñä      \u001b[0m| 19/50 [02:45<04:22,  8.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/50, Train Loss: 0.2872, Recon: 0.2871, Diffusion: 0.0000, Latent: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/50, Train Loss: 0.2863, Recon: 0.2863, Diffusion: 0.0000, Latent: 0.0000\n",
      "Validation - Loss: 0.2865, Recon: 0.2865, Diffusion: 0.0000, Latent: 0.0000\n",
      "üì∏ Saved train sample: ./outputs/pipeline_samples/train_epoch_20_sample_0.png\n",
      "üì∏ Saved train sample: ./outputs/pipeline_samples/train_epoch_20_sample_1.png\n",
      "üì∏ Saved train sample: ./outputs/pipeline_samples/train_epoch_20_sample_2.png\n",
      "üì∏ Saved train sample: ./outputs/pipeline_samples/train_epoch_20_sample_3.png\n",
      "üì∏ Saved val sample: ./outputs/pipeline_samples/val_epoch_20_sample_0.png\n",
      "üì∏ Saved val sample: ./outputs/pipeline_samples/val_epoch_20_sample_1.png\n",
      "üì∏ Saved val sample: ./outputs/pipeline_samples/val_epoch_20_sample_2.png\n",
      "üì∏ Saved val sample: ./outputs/pipeline_samples/val_epoch_20_sample_3.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  40%|\u001b[32m‚ñà‚ñà‚ñà‚ñà      \u001b[0m| 20/50 [02:59<05:01, 10.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saved checkpoint: ./outputs/pipeline_checkpoints/model_epoch_20.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  42%|\u001b[32m‚ñà‚ñà‚ñà‚ñà‚ñè     \u001b[0m| 21/50 [03:07<04:33,  9.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/50, Train Loss: 0.2860, Recon: 0.2860, Diffusion: 0.0000, Latent: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  44%|\u001b[32m‚ñà‚ñà‚ñà‚ñà‚ñç     \u001b[0m| 22/50 [03:15<04:12,  9.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/50, Train Loss: 0.2852, Recon: 0.2852, Diffusion: 0.0000, Latent: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  46%|\u001b[32m‚ñà‚ñà‚ñà‚ñà‚ñå     \u001b[0m| 23/50 [03:23<03:56,  8.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/50, Train Loss: 0.2849, Recon: 0.2849, Diffusion: 0.0000, Latent: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  48%|\u001b[32m‚ñà‚ñà‚ñà‚ñà‚ñä     \u001b[0m| 24/50 [03:33<03:52,  8.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/50, Train Loss: 0.2846, Recon: 0.2846, Diffusion: 0.0000, Latent: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/50, Train Loss: 0.2843, Recon: 0.2842, Diffusion: 0.0000, Latent: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  50%|\u001b[32m‚ñà‚ñà‚ñà‚ñà‚ñà     \u001b[0m| 25/50 [03:41<03:38,  8.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saved checkpoint: ./outputs/pipeline_checkpoints/model_epoch_25.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  52%|\u001b[32m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    \u001b[0m| 26/50 [03:49<03:26,  8.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/50, Train Loss: 0.2838, Recon: 0.2838, Diffusion: 0.0000, Latent: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  54%|\u001b[32m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    \u001b[0m| 27/50 [03:57<03:13,  8.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/50, Train Loss: 0.2837, Recon: 0.2837, Diffusion: 0.0000, Latent: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  56%|\u001b[32m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    \u001b[0m| 28/50 [04:07<03:12,  8.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/50, Train Loss: 0.2830, Recon: 0.2830, Diffusion: 0.0000, Latent: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  58%|\u001b[32m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    \u001b[0m| 29/50 [04:15<03:01,  8.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/50, Train Loss: 0.2830, Recon: 0.2830, Diffusion: 0.0000, Latent: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/50, Train Loss: 0.2827, Recon: 0.2827, Diffusion: 0.0000, Latent: 0.0000\n",
      "Validation - Loss: 0.2832, Recon: 0.2832, Diffusion: 0.0000, Latent: 0.0000\n",
      "üì∏ Saved train sample: ./outputs/pipeline_samples/train_epoch_30_sample_0.png\n",
      "üì∏ Saved train sample: ./outputs/pipeline_samples/train_epoch_30_sample_1.png\n",
      "üì∏ Saved train sample: ./outputs/pipeline_samples/train_epoch_30_sample_2.png\n",
      "üì∏ Saved train sample: ./outputs/pipeline_samples/train_epoch_30_sample_3.png\n",
      "üì∏ Saved val sample: ./outputs/pipeline_samples/val_epoch_30_sample_0.png\n",
      "üì∏ Saved val sample: ./outputs/pipeline_samples/val_epoch_30_sample_1.png\n",
      "üì∏ Saved val sample: ./outputs/pipeline_samples/val_epoch_30_sample_2.png\n",
      "üì∏ Saved val sample: ./outputs/pipeline_samples/val_epoch_30_sample_3.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  60%|\u001b[32m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    \u001b[0m| 30/50 [04:28<03:15,  9.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saved checkpoint: ./outputs/pipeline_checkpoints/model_epoch_30.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  62%|\u001b[32m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   \u001b[0m| 31/50 [04:37<03:05,  9.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/50, Train Loss: 0.2822, Recon: 0.2822, Diffusion: 0.0000, Latent: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  64%|\u001b[32m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   \u001b[0m| 32/50 [04:46<02:47,  9.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/50, Train Loss: 0.2820, Recon: 0.2820, Diffusion: 0.0000, Latent: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  66%|\u001b[32m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   \u001b[0m| 33/50 [04:54<02:33,  9.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/50, Train Loss: 0.2818, Recon: 0.2818, Diffusion: 0.0000, Latent: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  68%|\u001b[32m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   \u001b[0m| 34/50 [05:02<02:20,  8.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/50, Train Loss: 0.2814, Recon: 0.2814, Diffusion: 0.0000, Latent: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/50, Train Loss: 0.2811, Recon: 0.2811, Diffusion: 0.0000, Latent: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  70%|\u001b[32m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   \u001b[0m| 35/50 [05:12<02:16,  9.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saved checkpoint: ./outputs/pipeline_checkpoints/model_epoch_35.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  72%|\u001b[32m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  \u001b[0m| 36/50 [05:20<02:03,  8.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/50, Train Loss: 0.2812, Recon: 0.2812, Diffusion: 0.0000, Latent: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  74%|\u001b[32m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  \u001b[0m| 37/50 [05:28<01:51,  8.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/50, Train Loss: 0.2808, Recon: 0.2808, Diffusion: 0.0000, Latent: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  76%|\u001b[32m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  \u001b[0m| 38/50 [05:36<01:41,  8.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/50, Train Loss: 0.2806, Recon: 0.2806, Diffusion: 0.0000, Latent: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  78%|\u001b[32m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  \u001b[0m| 39/50 [05:46<01:35,  8.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/50, Train Loss: 0.2807, Recon: 0.2807, Diffusion: 0.0000, Latent: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/50, Train Loss: 0.2803, Recon: 0.2803, Diffusion: 0.0000, Latent: 0.0000\n",
      "Validation - Loss: 0.2807, Recon: 0.2807, Diffusion: 0.0000, Latent: 0.0000\n",
      "üì∏ Saved train sample: ./outputs/pipeline_samples/train_epoch_40_sample_0.png\n",
      "üì∏ Saved train sample: ./outputs/pipeline_samples/train_epoch_40_sample_1.png\n",
      "üì∏ Saved train sample: ./outputs/pipeline_samples/train_epoch_40_sample_2.png\n",
      "üì∏ Saved train sample: ./outputs/pipeline_samples/train_epoch_40_sample_3.png\n",
      "üì∏ Saved val sample: ./outputs/pipeline_samples/val_epoch_40_sample_0.png\n",
      "üì∏ Saved val sample: ./outputs/pipeline_samples/val_epoch_40_sample_1.png\n",
      "üì∏ Saved val sample: ./outputs/pipeline_samples/val_epoch_40_sample_2.png\n",
      "üì∏ Saved val sample: ./outputs/pipeline_samples/val_epoch_40_sample_3.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  80%|\u001b[32m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  \u001b[0m| 40/50 [05:58<01:37,  9.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saved checkpoint: ./outputs/pipeline_checkpoints/model_epoch_40.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  82%|\u001b[32m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè \u001b[0m| 41/50 [06:06<01:23,  9.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/50, Train Loss: 0.2801, Recon: 0.2801, Diffusion: 0.0000, Latent: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  84%|\u001b[32m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç \u001b[0m| 42/50 [06:14<01:10,  8.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/50, Train Loss: 0.2800, Recon: 0.2800, Diffusion: 0.0000, Latent: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  86%|\u001b[32m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå \u001b[0m| 43/50 [06:23<01:03,  9.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/50, Train Loss: 0.2798, Recon: 0.2798, Diffusion: 0.0000, Latent: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  88%|\u001b[32m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä \u001b[0m| 44/50 [06:31<00:52,  8.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/50, Train Loss: 0.2796, Recon: 0.2795, Diffusion: 0.0000, Latent: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/50, Train Loss: 0.2794, Recon: 0.2794, Diffusion: 0.0000, Latent: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  90%|\u001b[32m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà \u001b[0m| 45/50 [06:40<00:43,  8.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saved checkpoint: ./outputs/pipeline_checkpoints/model_epoch_45.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  92%|\u001b[32m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè\u001b[0m| 46/50 [06:48<00:33,  8.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/50, Train Loss: 0.2793, Recon: 0.2793, Diffusion: 0.0000, Latent: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  94%|\u001b[32m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç\u001b[0m| 47/50 [06:57<00:26,  8.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/50, Train Loss: 0.2792, Recon: 0.2792, Diffusion: 0.0000, Latent: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  96%|\u001b[32m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå\u001b[0m| 48/50 [07:05<00:17,  8.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/50, Train Loss: 0.2790, Recon: 0.2790, Diffusion: 0.0000, Latent: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  98%|\u001b[32m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä\u001b[0m| 49/50 [07:13<00:08,  8.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/50, Train Loss: 0.2788, Recon: 0.2788, Diffusion: 0.0000, Latent: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50, Train Loss: 0.2787, Recon: 0.2787, Diffusion: 0.0000, Latent: 0.0000\n",
      "Validation - Loss: 0.2790, Recon: 0.2790, Diffusion: 0.0000, Latent: 0.0000\n",
      "üì∏ Saved train sample: ./outputs/pipeline_samples/train_epoch_50_sample_0.png\n",
      "üì∏ Saved train sample: ./outputs/pipeline_samples/train_epoch_50_sample_1.png\n",
      "üì∏ Saved train sample: ./outputs/pipeline_samples/train_epoch_50_sample_2.png\n",
      "üì∏ Saved train sample: ./outputs/pipeline_samples/train_epoch_50_sample_3.png\n",
      "üì∏ Saved val sample: ./outputs/pipeline_samples/val_epoch_50_sample_0.png\n",
      "üì∏ Saved val sample: ./outputs/pipeline_samples/val_epoch_50_sample_1.png\n",
      "üì∏ Saved val sample: ./outputs/pipeline_samples/val_epoch_50_sample_2.png\n",
      "üì∏ Saved val sample: ./outputs/pipeline_samples/val_epoch_50_sample_3.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|\u001b[32m‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\u001b[0m| 50/50 [07:27<00:00,  8.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saved checkpoint: ./outputs/pipeline_checkpoints/model_epoch_50.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saved final checkpoint: ./outputs/pipeline_checkpoints/model_final.pth\n",
      "‚úÖ Training completed!\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "train_model(model, train_loader, val_loader, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bhavik_mini",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
